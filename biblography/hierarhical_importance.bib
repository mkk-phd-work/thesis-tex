@article{cond_pfi,
    title = {Model-agnostic Feature Importance and Effects with Dependent Features - A Conditional Subgroup Approach.},
    DOI = {10.1007/s10618-022-00901-9},
    journal = {Data mining and knowledge discovery},
    author = {Christoph Molnar and Molnar, Christoph and Gunnar König and König, Gunnar and Bernd Bischl and Bischl, Bernd and Giuseppe Casalicchio and Casalicchio, Giuseppe},
    year = {2020},
    month = jun }

@article{scott_lundberg_unified_2017-1,
    title = {A unified approach to interpreting model predictions},
    volume = {30},
    abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
    author = {{Scott Lundberg} and Lundberg, Scott and {Su-In Lee} and Lee, Su-In},
    month = dec,
    year = {2017},
    pages = {4768--4777},
}

@article{scott_lundberg_consistent_2018,
    title = {Consistent {Individualized} {Feature} {Attribution} for {Tree} {Ensembles}.},
    abstract = {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique "supervised" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see this http URL for details.},
    journal = {arXiv: Learning},
    author = {{Scott Lundberg} and Lundberg, Scott and {Gabriel Erion} and Erion, Gabriel and {Su-In Lee} and Lee, Su-In},
    month = feb,
    year = {2018},

}

@article{MAKRIDAKIS2018802,
    title = {The M4 Competition: Results, findings, conclusion and way forward},
    journal = {International Journal of Forecasting},
    volume = {34},
    number = {4},
    pages = {802-808},
    year = {2018},
    issn = {0169-2070},
    doi = {https://doi.org/10.1016/j.ijforecast.2018.06.001},
    url = {https://www.sciencedirect.com/science/article/pii/S0169207018300785},
    author = {Spyros Makridakis and Evangelos Spiliotis and Vassilios Assimakopoulos},
    keywords = {Forecasting competitions, M Competitions, Forecasting accuracy, Prediction intervals (PIs), Time series methods, Machine Learning (ML) methods, Benchmarking methods, Practice of forecasting},
}

@article{doshi, author = {Doshi‐Velez, F. and Kim, B.}, title = {Considerations for evaluation and generalization in interpretable machine learning}, journal = {The Springer Series on Challenges in Machine Learning}, year = {2018}, pages = {3-17}, doi = {10.1007/978-3-319-98131-4_1} }

@book{hyndman2018forecasting,
    title = {Forecasting: principles and practice},
    edition = {3},
    author = {Hyndman, Rob J and Athanasopoulos, George},
    year = {2021},
    publisher = {OTexts},
    url = {https://otexts.com/fpp3/index.html},
}

@ARTICLE{Saluja_2021,
    author = {Saluja, Rohit and Malhi, Avleen and Knapič, Samanta and Främling, Kary and Cavdar, Cicek},
    year = {2021},
    month = {04},
    pages = {-},
    title = {Towards a Rigorous Evaluation of Explainability for Multivariate Time Series},
    doi = {10.2139/ssrn.4627337}
}

@inproceedings{fahse2022explanation,
    title = {Explanation Interfaces for Sales Forecasting},
    author = {Fahse, Tobias and Blohm, Ivo and Hruby, Richard and van Giffen, Benjamin},
    booktitle = { ECIS 2022 Research-in-Progress Papers},
    year = {2022},
    month = {03},

}

@book{vandeput2023demand,
    title = {Demand forecasting best practices},
    isbn = {978-1-63835-197-9},
    url = {https://books.google.ro/books?id=C_u8EAAAQBAJ},
    publisher = {Manning},
    author = {Vandeput, N.},
    year = {2023},
}

@misc{joachim2023demand,
    title = {Global forecasting models: Dependent multi-series forecasting (multivariate forecasting)},
    url = {https://skforecast.org/0.12.1/user_guides/dependent-multi-series-multivariate-forecasting.html},
    journal = {Global Models: Dependent multivariate series forecasting - Skforecast Docs},
    author = {Joaquin Amat Rodrigo and Javier Escobar Ortiz},
    year = {2024}
}

@software{skforecast,
    author = {Amat Rodrigo, Joaquin and Escobar Ortiz, Javier},
    title = {skforecast},
    version = {0.13.0},
    month = {8},
    year = {2024},
    license = {BSD-3-Clause},
    url = {https://skforecast.org/},
    key = {value},doi = {10.5281/zenodo.8382788}
}


@inproceedings{scott_lundberg_unified_2017,
    author = {Lundberg, Scott M. and Lee, Su-In},
    title = {A unified approach to interpreting model predictions},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {4768–4777},
    numpages = {10},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}

@misc{favorita-sales,
    author = {Corporación Favorita inversion, Julia Elliott, Mark McDonald},
    title = {Corporación Favorita Grocery Sales Forecasting},
    publisher = {Kaggle},
    year = {2017},
    url = {https://kaggle.com/competitions/favorita-grocery-sales-forecasting}
}

@article{jamal_fattah_forecasting_2018,
    author = {Jamal Fattah and Latifa Ezzine and Zineb Aman and Haj El Moussami and Abdeslam Lachhab},
    title = {Forecasting of demand using ARIMA model},
    journal = {International Journal of Engineering Business Management},
    volume = {10},
    number = {},
    pages = {1847979018808673},
    year = {2018},
    doi = {10.1177/1847979018808673},
    URL = {    https://doi.org/10.1177/1847979018808673    },
    eprint = {    https://doi.org/10.1177/1847979018808673}
}

@inproceedings{ingle2021demand,
    title = {Demand forecasting: Literature review on various methodologies},
    author = {Ingle, Chaitanya and Bakliwal, Dev and Jain, Jayesh and Singh, Preeyesh and Kale, Preeti and Chhajed, Vaibhav},
    booktitle = {2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)},
    pages = {1--7},
    year = {2021},
    organization = {IEEE}
}

@article{spiliotis2022comparison,
    title = {Comparison of statistical and machine learning methods for daily SKU demand forecasting},
    author = {Spiliotis, Evangelos and Makridakis, Spyros and Semenoglou, Artemios-Anargyros and Assimakopoulos, Vassilios},
    journal = {Operational Research},
    volume = {22},
    number = {3},
    pages = {3037--3061},
    year = {2022},
    publisher = {Springer}
}

@article{MONTEROMANSO202086,
    title = {FFORMA: Feature-based forecast model averaging},
    journal = {International Journal of Forecasting},
    volume = {36},
    number = {1},
    pages = {86-92},
    year = {2020},
    note = {M4 Competition},
    issn = {0169-2070},
    doi = {https://doi.org/10.1016/j.ijforecast.2019.02.011},
    url = {https://www.sciencedirect.com/science/article/pii/S0169207019300895},

}

@inproceedings{optuna_2019,
    title = {Optuna: A Next-generation Hyperparameter Optimization Framework},
    author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
    booktitle = {Proceedings of the 25th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining},
    year = {2019}
}

@Article{Breiman2001,
    author = {Breiman, Leo},
    title = {Random Forests},
    journal = {Machine Learning},
    year = {2001},
    month = {Oct},
    day = {01},
    volume = {45},
    number = {1},
    pages = {5-32},
    abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
    issn = {1573-0565},
    url = {https://doi.org/10.1023/A:1010933404324}
}

@book{Molnar2022,
    title = {Interpretable Machine Learning},
    author = {Christoph Molnar},
    year = {2022},
    subtitle = {A Guide for Making Black Box Models Explainable},
    edition = {2},
    publisher = {online},
    url = {https://christophm.github.io/interpretable-ml-book}
}

@ARTICLE{Molnar_2020_pitfalls,
    title = {Pitfalls to Avoid when Interpreting Machine Learning Models},
    year = {2020},
    author = {Christoph Molnar and Christoph Molnar and Gunnar König and Gunnar König and Julia Herbinger and Julia Herbinger and Timo Freiesleben and Timo Freiesleben and Susanne Dandl and Susanne Dandl and Christian A. Scholbeck and Christian A. Scholbeck and Giuseppe Casalicchio and Giuseppe Casalicchio and Moritz Grosse-Wentrup and Moritz Grosse-Wentrup and Bernd Bischl and Bernd Bischl},
    doi = {null},pmid = {null},
    pmcid = {null},mag_id = {3041627266},
    journal = {arXiv.org},
    abstract = {Modern requirements for machine learning (ML) models include both high predictive performance and model interpretability. A growing number of techniques provide model interpretations, but can lead to wrong conclusions if applied incorrectly. We illustrate pitfalls of ML model interpretation such as bad model generalization, dependent features, feature interactions or unjustified causal interpretations. Our paper addresses ML practitioners by raising awareness of pitfalls and pointing out solutions for correct model interpretation, as well as ML researchers by discussing open issues for further research.}
}


@article{Guidotti2018,
    author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
    title = {A Survey of Methods for Explaining Black Box Models},
    year = {2018},
    issue_date = {September 2019},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {51},
    number = {5},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3236009},
    doi = {10.1145/3236009},
    abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
    journal = {ACM Comput. Surv.},
    month = {aug},
    articleno = {93},
    numpages = {42},
    keywords = {transparent models, Open the black box, explanations, interpretability}
}

@article{tianqi_chen_xgboost_2016,
    title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
    doi = {10.1145/2939672.2939785},
    abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
    journal = {arXiv: Learning},
    author = {{Tianqi Chen} and Chen, Tianqi and {Carlos Guestrin} and Guestrin, Carlos},
    year = {2016}

}

@article{leo_breiman_random_2001,
    title = {Random {Forests}},
    volume = {45},
    doi = {10.1023/a:1010933404324},
    abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, aaa, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
    number = {1},
    journal = {Machine-mediated learning},
    author = {{Leo Breiman} and Breiman, Leo},
    month = oct,
    year = {2001},
    pages = {5--32},
}

@article{vikas_c_raykar_tsshap_2023,
    title = {{TsSHAP}: {Robust} model agnostic feature-based explainability for time
 series forecasting},
    doi = {10.48550/arxiv.2303.12316},
    abstract = {A trustworthy machine learning model should be accurate as well as explainable. Understanding why a model makes a certain decision defines the notion of explainability. While various flavors of explainability have been well-studied in supervised learning paradigms like classification and regression, literature on explainability for time series forecasting is relatively scarce. In this paper, we propose a feature-based explainability algorithm, TsSHAP, that can explain the forecast of any black-box forecasting model. The method is agnostic of the forecasting model and can provide explanations for a forecast in terms of interpretable features defined by the user a prior. The explanations are in terms of the SHAP values obtained by applying the TreeSHAP algorithm on a surrogate model that learns a mapping between the interpretable feature space and the forecast of the black-box model. Moreover, we formalize the notion of local, semi-local, and global explanations in the context of time series forecasting, which can be useful in several scenarios. We validate the efficacy and robustness of TsSHAP through extensive experiments on multiple datasets.},
    journal = {arXiv.org},
    author = {{Vikas C. Raykar} and {Arindam Jati} and {Sumanta Mukherjee} and {Nupur Aggarwal} and {Kanthi K. Sarpatwar} and {Giridhar Ganapavarapu} and {Roman Vaculín}},
    month = mar,
    year = {2023},
    note = {ARXIV\_ID: 2303.12316},
}

@article{scott_lundberg_local_2020,
    title = {From {Local} {Explanations} to {Global} {Understanding} with {Explainable} {AI} for {Trees}.},
    volume = {2},
    doi = {10.1038/s42256-019-0138-9},
    abstract = {Tree-based machine learning models such as random forests, decision trees and gradient boosted trees are popular nonlinear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here we improve the interpretability of tree-based models through three main contributions. (1) A polynomial time algorithm to compute optimal explanations based on game theory. (2) A new type of explanation that directly measures local feature interaction effects. (3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to (1) identify high-magnitude but low-frequency nonlinear mortality risk factors in the US population, (2) highlight distinct population subgroups with shared risk characteristics, (3) identify nonlinear interaction effects among risk factors for chronic kidney disease and (4) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model’s performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains. Tree-based machine learning models are widely used in domains such as healthcare, finance and public services. The authors present an explanation method for trees that enables the computation of optimal local explanations for individual predictions, and demonstrate their method on three medical datasets.},
    number = {1},
    journal = {Nature Machine Intelligence},
    author = {{Scott Lundberg} and Lundberg, Scott and {Gabriel Erion} and Erion, Gabriel and {Hugh Chen} and Chen, Hugh and {Alex J. DeGrave} and DeGrave, Alex J. and {Jordan M. Prutkin} and Prutkin, Jordan M. and {Bala G. Nair} and Nair, Bala G. and {Ronit Katz} and Katz, Ronit and {Jonathan Himmelfarb} and Himmelfarb, Jonathan and Flynn, Joseph T. and {Nisha Bansal} and Bansal, Nisha and {Su-In Lee} and Lee, Su-In},
    month = jan,
    year = {2020},

    pmid = {32607472},

    pages = {56--67},
}

@article{ian_covert_understanding_2020,
    title = {Understanding {Global} {Feature} {Contributions} {With} {Additive} {Importance} {Measures}},
    volume = {33},
    abstract = {Understanding the inner workings of complex machine learning models is a long-standing problem and most recent research has focused on local interpretability. To assess the role of individual input features in a global sense, we explore the perspective of defining feature importance through the predictive power associated with each feature. We introduce two notions of predictive power (model-based and universal) and formalize this approach with a framework of additive importance measures, which unifies numerous methods in the literature. We then propose SAGE, a model-agnostic method that quantifies predictive power while accounting for feature interactions. Our experiments show that SAGE can be calculated efficiently and that it assigns more accurate importance values than other methods.},
    journal = {Neural Information Processing Systems},
    author = {{Ian Covert} and Covert, Ian and {Scott Lundberg} and Lundberg, Scott and {Su-In Lee} and Lee, Su-In},
    year = {2020},

    pages = {17212--17223},
}

@article{christophmolnarModelagnosticFeatureImportance2020,
    title = {Model-agnostic {Feature} {Importance} and {Effects} with {Dependent} {Features} - {A} {Conditional} {Subgroup} {Approach}.},
    doi = {10.1007/s10618-022-00901-9},
    abstract = {Partial dependence plots and permutation feature importance are popular model-agnostic interpretation methods. Both methods are based on predicting artificially created data points. When features are dependent, both methods extrapolate to feature areas with low data density. The extrapolation can cause misleading interpretations. To overcome extrapolation, we propose conditional variants of partial dependence plots and permutation feature importance. Our approach is based on perturbations in subgroups. The subgroups partition the feature space to make the feature distribution within a group more homogeneous and between the groups more heterogeneous. The interpretable subgroups enable additional local, nuanced interpretations of the feature dependence structure as well as the feature effects and importance values within the subgroups. We also introduce a data fidelity measure that captures the degree of extrapolation when data is transformed with a certain perturbation. In simulations and benchmarks on real data we show that our conditional interpretation methods reduce extrapolation. In an application we show that these methods provide more nuanced and richer explanations.},
    journal = {Data mining and knowledge discovery},
    author = {{Christoph Molnar} and Molnar, Christoph and {Gunnar König} and König, Gunnar and {Bernd Bischl} and Bischl, Bernd and {Giuseppe Casalicchio} and Casalicchio, Giuseppe},
    month = jun,
    year = {2020},
    note = {ARXIV\_ID: 2006.04628
MAG ID: 3033797488
}
}

@article{makridakis_m5_2022,
    title = {M5 accuracy competition: {Results}, findings, and conclusions},
    volume = {38},
    issn = {0169-2070},
    url = {https://www.sciencedirect.com/science/article/pii/S0169207021001874},
    doi = {https://doi.org/10.1016/j.ijforecast.2021.11.013},
    abstract = {In this study, we present the results of the M5 “Accuracy” competition, which was the first of two parallel challenges in the latest M competition with the aim of advancing the theory and practice of forecasting. The main objective in the M5 “Accuracy” competition was to accurately predict 42,840 time series representing the hierarchical unit sales for the largest retail company in the world by revenue, Walmart. The competition required the submission of 30,490 point forecasts for the lowest cross-sectional aggregation level of the data, which could then be summed up accordingly to estimate forecasts for the remaining upward levels. We provide details of the implementation of the M5 “Accuracy” challenge, as well as the results and best performing methods, and summarize the major findings and conclusions. Finally, we discuss the implications of these findings and suggest directions for future research.},
    number = {4},
    journal = {International Journal of Forecasting},
    author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
    year = {2022},
    keywords = {Accuracy, Forecasting competitions, M competitions, Machine learning, Retail sales forecasting, Time series},
    pages = {1346--1364},
}

@misc{molnar2020limitations,
    title = {Limitations of interpretable machine learning methods},
    author = {Molnar, Christoph and Gruber, S and Kopper, P},
    year = {2020},
    url = {https://slds-lmu.github.io/iml_methods_limitations/},
    note = {MAG ID: 3041627266},

}







@article{giles_hooker_unrestricted_2021,
    title = {Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance},
    volume = {31},
    doi = {10.1007/s11222-021-10057-z},
    number = {6},
    journal = {Statistics and Computing},
    author = {{Giles Hooker} and Hooker, Giles and {Giles Hooker} and {Lucas Mentch} and Mentch, Lucas and {Siyu Zhou} and Zhou, Siyu},
    month = nov,
    year = {2021},


    pages = {1--16},
}


@article{kristin_blesch_conditional_2023,
    title = {Conditional feature importance for mixed data},
    doi = {10.1007/s10182-023-00477-9},
    abstract = {Despite the popularity of feature importance (FI) measures in interpretable machine learning, the statistical adequacy of these methods is rarely discussed. From a statistical perspective, a major distinction is between analyzing a variable's importance before and after adjusting for covariates - i.e., between \${\textbackslash}textit\{marginal\}\$ and \${\textbackslash}textit\{conditional\}\$ measures. Our work draws attention to this rarely acknowledged, yet crucial distinction and showcases its implications. Further, we reveal that for testing conditional FI, only few methods are available and practitioners have hitherto been severely restricted in method application due to mismatching data requirements. Most real-world data exhibits complex feature dependencies and incorporates both continuous and categorical data (mixed data). Both properties are oftentimes neglected by conditional FI measures. To fill this gap, we propose to combine the conditional predictive impact (CPI) framework with sequential knockoff sampling. The CPI enables conditional FI measurement that controls for any feature dependencies by sampling valid knockoffs - hence, generating synthetic data with similar statistical properties - for the data to be analyzed. Sequential knockoffs were deliberately designed to handle mixed data and thus allow us to extend the CPI approach to such datasets. We demonstrate through numerous simulations and a real-world example that our proposed workflow controls type I error, achieves high power and is in line with results given by other conditional FI measures, whereas marginal FI metrics result in misleading interpretations. Our findings highlight the necessity of developing statistically adequate, specialized methods for mixed data.},
    journal = {AStA Advances in Statistical Analysis},
    author = {{Kristin Blesch} and {David S. Watson} and {Marvin N. Wright}},
    month = apr,
    year = {2023},

}

@article{gordon_classification_1984,
    title = {classification and regression trees},
    doi = {10.2307/2530946},
    abstract = {Bellows TS and Fisher TW (eds.) (1999) Handbook of Biological Control: Principles and Applications of Biological Control. San Diego: Academic Press. Clausen CP (ed.) (1978) Agricultural Research Service: Handbook No. 480: Introduced Parasites and Predators of Arthropod Pests and Weeds: A World Review. Washington, DC: USDA: Agricultural Research Service. DeBach P and Rosen D (1991) Biological Control by Natural Enemies. Cambridge, UK: Cambridge University Press. Follett PA and Duan JJ (eds.) (2000) Nontarget Effects of Biological Control. Boston, MA: Kluwer Academic. Jervis M and Kidd N (eds.) (1996) Insect Natural Enemies: Practical Approaches to their Study and Evaluation. London: Chapman and Hall. Julien MH and Griffiths MW (eds.) (1998) Biological Control of Weeds, a World Catalogue of Agents and their Target Weeds, 4th edn. Wallingford: CABI Publishing. Van Driesche J and Van Driesche RG (2000) Nature Out of Place: Biological Invasions in a Global Age. Washington, DC: Island Press. Van Driesche RG, Hoddle M, and Center T (2008) Control of Pests and Weeds by Natural Enemies, an Introduction to Biological Control. London: Blackwell.},
    journal = {Biometrics},
    author = {Gordon, A. and Breiman, Leo and {Jerome H. Friedman} and Olshen, Richard A. and {Charles J. Stone}},
    year = {1984},

}


