


@Article{Linardatos2021,
    AUTHOR = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
    TITLE = {Explainable AI: A Review of Machine Learning Interpretability Methods},
    JOURNAL = {Entropy},
    VOLUME = {23},
    YEAR = {2021},
    NUMBER = {1},
    ARTICLE-NUMBER = {18},
    URL = {https://www.mdpi.com/1099-4300/23/1/18},
    PubMedID = {33375658},
    ISSN = {1099-4300},
    DOI = {10.3390/e23010018}
}

@article{Guidotti2018ASO,
    title={A Survey of Methods for Explaining Black Box Models},
    author={Riccardo Guidotti and Anna Monreale and Franco Turini and Dino Pedreschi and Fosca Giannotti},
    journal={ACM Computing Surveys (CSUR)},
    year={2018},
    volume={51},
    pages={1 - 42},
    url={https://api.semanticscholar.org/CorpusID:3342225}
}
@inproceedings{gilpin2018explaining,
    title={Explaining explanations: An overview of interpretability of machine learning},
    author={Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
    booktitle={2018 IEEE 5th International Conference on data science and advanced analytics (DSAA)},
    pages={80--89},
    year={2018},
    organization={IEEE}
}

@article{dictionary2017cambridge,
    title={Cambridge advanced learner’s dictionary \& thesaurus},
    author={Dictionary, Cambridge},
    journal={Retrieved from Cambridge website: https://dictionary. cambridge. org/dictionary/english},
    year={2017}
}


@article{BARREDOARRIETA202082,
    title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
    journal = {Information Fusion},
    volume = {58},
    pages = {82-115},
    year = {2020},
    issn = {1566-2535},
    doi = {https://doi.org/10.1016/j.inffus.2019.12.012},
    url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
    author = {Alejandro {Barredo Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
}



@article {Finale2017,
    title = {Towards A Rigorous Science of Interpretable Machine Learning},
    year = {2017},
    abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
    url = {https://arxiv.org/pdf/1702.08608.pdf},
    author = {Doshi-Velez, Finale and Been Kim}
}

@article{miller2019explanation,
    title={Explanation in artificial intelligence: Insights from the social sciences},
    author={Miller, Tim},
    journal={Artificial intelligence},
    volume={267},
    pages={1--38},
    year={2019},
    publisher={Elsevier}
}

@article{rudin2019stop,
    title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
    author={Rudin, Cynthia},
    journal={Nature machine intelligence},
    volume={1},
    number={5},
    pages={206--215},
    year={2019},
    publisher={Nature Publishing Group UK London}
}

