To evaluate different scenarios on the simulated datasets, multiple experiments were conducted with LightGBM and Random Forest models to evaluate the feature importance methods.
The main implementations of the FI method were the Permutation Feature Importance (PFI) of sklearn\footnote{sklearn - Permutation Feature Importance: \url{https://scikit-learn.org/stable/modules/permutation_importance.html}}, the TreeExplainer SHAP from fasttreeshap\footnote{fasttreeshap - TreeExplainer: \url{https://github.com/linkedin/FastTreeSHAP}}, and the built-in Tree Gain and Split Importance from LightGBM\footnote{LightGBM - Feature Importance:\url{https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html}}, with only the Tree Gain used for Random Forest\footnote{Random Forest - Feature Importance: \url{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}}.